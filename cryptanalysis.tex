\section{Cryptanalysis}

\outlinecomment{Get Aono, Moriai or others to contribute here.}

The general idea of trying to decode messages that are encrypted is
known as cryptanalysis.  Some of the planet's smartest people have
worked on this for all of their lifetimes, building on work in
progress since well before we were born, so my summary here is
inevitably going to be superficial and perhaps wrong.  Comments
welcome; there is a lot of \emph{bad} information about cryptography on the
web, and I'd prefer to be corrected and make mine \emph{good} information
for the web.

(Somewhere, right now, maybe even as I type, employees of one or more
of the world's major intelligence agencies are snickering at how naive
my presentation here is.  We don't know how much they really know, but
we do know that e.g. the RSA public key cryptosystem and the technique
of differential analysis were known to U.S. agencies for years,
perhaps decades, before they became public.)

\url{https://en.wikipedia.org/wiki/Advanced_Encryption_Standard#Known_attacks}
says, "For cryptographers, a cryptographic "break" is anything faster
than a brute-force attack â€“ i.e., performing one trial decryption for
each possible key in sequence (see Cryptanalysis). A break can thus
include results that are infeasible with current technology."
The page then goes on to list quite a number of attacks on AES, none
considered practical yet.

Given how long people have been working on cryptography, naturally
there are many techniques for attacking the ciphers.  Here I'll
mention just a couple of the modern techniques are known for what we
might call "honest" attacks on block cipher cryptography, granting the
assumption that the algorithm has no flaws and ignoring side channel
attacks (such as measuring the power consumed during encryption) and
attacks on infrastructure, people, etc. (some of which are known as
"black bag", or burglary, attacks, and some of which are "rubber hose"
attacks, extracting information from humans by coercion or torture).
David Kahn referred to these latter attacks as "practical
cryptanalysis" in his classic book, \emph{The Codebreakers}.

The older one of the two is differential cryptanalysis, apparently
discovered indepedently at least three times, publicly by Biham and
Shamir in the late 1980s, in the mid-70s by IBM (where the goal of
defending against the technique drove DES design decisions), and
earlier by NSA.
\url{https://en.wikipedia.org/wiki/Differential_cryptanalysis}
The second is linear crypanalysis, discovered by Mitsuru Matsui in
1990~\cite{matsui1993linear}.
\url{https://en.wikipedia.org/wiki/Linear_cryptanalysis}

(Other techniques that I know nothing about include integral,
algebraic, and biclique.)

Before talking about those two in any detail, there's a more
straightforward technique that leaks information about the plaintext,
if not the key itself: the birthday paradox.  But before we get into
the cryptanalysis itself, let's look a little bit at how the designers
built defenses into the block ciphers.

\subsection{Playing Defense Using Math}

\subsubsection{Entropy}

The single most important concept in cryptography -- indeed, in all of
information theory, and inarguably one of the most important in all of
computer science -- is Claude Shannon's \emph{entropy}.  The entropy is,
roughly, the amount of disorder in a sequence of data, or the amount
of information that must be transmitted to reconstruct the data.  When
the entropy of a cleartext message is high, it is hard to predict the
next symbol (bit or byte or letter or floating point number); purely
random data has very high entropy.  When it is low, the cleartext has
a skewed probability distribution such that, e.g., the letter 'e' or
the number 0 is more common than other values.

Of course, because encryption tries to hide information, encrypted
data looks as random as possible: it has very high entropy.  Thus, an
easy way to automate a test to see if we have successfully decrypted a
message is to calculate the entropy of our provisionally decrypted
plaintext; low entropy indicates a high probability that we have (at
least partially) succeeded.

The entropy is defined as
\begin{align}
H_{2}(X)=-\sum_{i=1}^{N} \frac{\operatorname{count}_{i}}{N} \log
_{2}\left(\frac{\operatorname{count}_{i}}{N}\right)
\end{align}
where $\operatorname{count}_i$ is the number of times that the $i$th
value appeared in our data, and $N$ is the number of different values
that show up in our data sequence.

Note that this definition of entropy does not take into account any
long-range structure; it is purely based on the overall probability of
the appearance of a given set of symbols.  Thus, to this function, the
strings 0101...01 and 000...01...111 have a bit-level entropy just as
high as a string composed of exactly 50/50 randomly chosen 0s and 1s,
although by applying a little intelligence we can easily reconstruct
the former but not the latter (at least, not exactly).  Many lossless
data compression programs operate on stateful \emph{sequences} of data
to predict the next symbol.  For example, the common Unix utilities
gzip and compress use the Lempel-Ziv
algorithm~\cite{compression:Ziv78}, which builds a dictionary of
sequences as it goes, so that repetitive data and long runs are
compressed effectively; this is especially good at sequences such as
0101...01 or 000...01...111.

(Taking still further advantage of brainpower, the ultimate in
compression is measured using the \emph{Kolmogorov complexity}: the
length of the shortest program that will reproduce the sequence.  This
has very low complexity for anything highly regular, as well as
seemingly irregular sequences such as the output of a pseudo-random
number generator or the digits of $\pi$.  I am not aware of any
automated form for calculating the Kolmogorov complexity of an
arbitrary sequence of data; in fact, it is known to be equivalent to
the halting problem~\cite{chaitin1975theory,chaitin1995program}.)

Many other tests for randomness are important and are used to validate
experimentally how completely an encryption system hides its data.  If
any pattern at all shows up in the ciphertext, that can be leveraged
in an attack, and is an indication that the encryption scheme is
poor.

This, obviously, is one of the motivations for the substitution phase
in a substitution-permutation network; with permutation only the
entropy of the original cleartext is preserved in the ciphertext, and
in some cases reveals a great deal about the original data.

\subsubsection{Diffusion, Confusion and the Avalanche Effect}
\label{sec:diffusion}

Claude Shannon, in his seminal article on the mathematics of
cryptography~\cite{shannon45}, defined two concepts he called
\emph{diffusion} and \emph{confusion}~\footnote{\url{https://en.wikipedia.org/wiki/Confusion_and_diffusion}}.  In diffusion, information from
the original plaintext message is spread across more than one symbol
in the output ciphertext; the farther the information spreads, the
better.  Shannon defined \emph{confusion} as making "the relation
between...[the ciphertext] $E$ and the...[key] $K$ a very complex and
involved one."

Feistel, who designed DES, called diffusion the \emph{avalanche
  effect}.  Webster and Tavares~\cite{webster86:avalanche} defined the
\emph{strict avalanche criterion} (SAC), requiring that changing a
single input bit flips each of the output bits with 50\% probability.

\emph{The Handbook of Applied Cryptography}~\cite{menezes1996handbook}
says the following (p. 20 in my edition):

\begin{quote}
A substitution in a round is said to add \emph{confusion} to the
encryption process whereas a transposition [permutation] is said to
add \emph{diffusion}. Confusion is intended to make the relationship
between the key and the ciphertext as complex as possible.  Diffusion
refers to rearranging or spreading out the bits in the message so that
any redundancy in the plaintext is spread out over the ciphertext.
\end{quote}

I haven't seen it written quite this directly (but then, I'm not that
well read in crypto), but I think it's fair to say that confusion is
achieved by the nonlinearity of the S-boxes.

These two concepts don't seem to figure prominently in the
cryptography books and papers I have been reading, but it seems to me
that they ultimately underly much of the process of cryptanalysis:
spreading data broadly reduces the relationship exhibited by two
ciphertexts even when the plaintexts are closely related, expanding
the search space; and the nonlinearity means that even a large set of
straightforward equations is not enough to simply and mathematically
relate the input and output.

\subsection{The Birthday Paradox}

(Also called the pigeonhole principle or the hash collision problem.)

You're probably familiar with the basic idea of the birthday paradox:
If you have $n$ people in a room, what's the probability of two of
them sharing a birthday?  More concretely, how many people have to be
in the room before the probability of two sharing a birthday exceeds
50\%?  It's a surprisingly small number, but it's not a true paradox in
the logical sense.  (This is also called the pigeonhole principle or
hash collision probability.)

Modern ciphers are designed so that the ciphertext (output of the
encryption) looks random; we can say that the ciphertext has very high
entropy.  If the block size is $n$ bits, each of the $2^n$ possible
bit combinations should be an equally likely result of encrypting a
data block.  (The encryption is deterministic, but the output looks
random.)

If we monitor a long stream of encrypted data, there is some
probability that we will find two blocks that have the same
ciphertext.  We call that a \emph{collision}.  If the two blocks were
encrypted with the same key, we gain information about the plaintext.

(Side question: How can you \emph{have} birthday paradox collisions of the
ciphertext?  Wouldn't that mean that the encryption process is lossy,
and that it would be impossible to reliably *decrypt* that ciphertext
back to both original blocks?

Answer: there is more information involved in both the encryption and
decryption than just that specific block and the key.  This is the
cipher block chaining mentioned at the end of Sec. 1.)

In CBC mode, what you gain is the XOR of two plaintext blocks.  If you
get to choose which two, this could be incredibly valuable
information; given that it's just two random blocks, it's less so, but
not unimportant.  As it happens, if block numbers $i$ and $j$ have the
same ciphertext $c[i] = c[j]$, then you get the XOR of the plaintext of
blocks $i-1$ and $j-1$.

I spent quite a bit of time working through the math of the birthday
paradox, only to come back to one of the first sources I found: the
2016 SWEET32 paper by Bhargavan and Leurent.  Sec. 2.2 of that paper
has a compact description of the commonly-quoted square root limit, as
well as why and how much to be conservative relative to that value.

If the block size is $n$ bits, there are $N = 2^n$ possible
ciphertexts, and if the number of blocks encrypted is the square root
of that number, $2^{n/2}$, the probability of at least one collision
is above 39\%, which arises in the limit as the expression
$1-1/e^{1/2}$.  (See the appendix of these notes for some example
calculations of this and the following to play around with.)

Assuming you consider a 39\% chance of disclosing some information to
be an unacceptably large probability, when should you rekey?  That's
the first-level answer to our ultimate question of when to rekey using
QKD, which is of course back where we started this conversation.  Say,
for example, we want the probability of an attacker recovering a
plaintext block using the birthday attack to be less than (for
example) one in a billion.

If we have some power of two $D = 2^d$ ciphertext blocks, then the
expected number of collisions is approximately $2^{2d-n-1}$.  For our
one-in-a-billion probability, using $log_2(10^9) \approx 30$, we need
to set up our session lifetime such that $2d-n-1 < -30$, or $d <
(n-29)/2$.

Since DES has only a 64-bit block, we should set $d \le 17$.  That's a
startlingly small number: $D = 2^{17}$ blocks and each block is eight
bytes, so we should limit the use of a single key to one megabyte!
An impractically small size.

If, on the other hand, you are okay with a disclosure probability of
one in a million, we can raise that by a factor of a thousand and
change keys once every gigabyte instead.  But if you are trying to
protect a 40Gbps link -- a common backbone bandwidth today, and coming
to the home in the foreseeable future -- that still means changing
keys once every 200msec or so!

Ah, but AES uses $n = 128$, a block size twice as large.  Now we only
need $d \le 49$ for that one-in-a-billion probability.  $D = 2^{49}$
times our block size of 16 bytes equals 8 terabytes, about 1,600
seconds on our 40Gbps link.  So change the keys at least once every
half hour, and you're good.

Keep in mind a few points:
\begin{enumerate}
\item what's disclosed here is not the key but is plaintext-related, but
   not even pure plaintext; however, the insanely smart and devious
   cryptanalysts can do amazing things with small amounts of data;
\item this depends only on the block size of a block cipher, not on the
   key length;
\item part of this arises due to the CBC (cipher block chain) mode, but
   ECB (electronic code book) mode is worse; there are other modes
   that I haven't looked at closely; and
\item given that there are still other attacks, minimizing the amount of
   data under any given encryption key is still good practice.
\end{enumerate}

So let's look at a couple of the cryptanalysis techniques.

\subsection{Differential Cryptanalysis}

Biham and Shamir wrote a seminal paper~\cite{biham1991differential},
then an easy-to-read book~\cite{biham1993differential-book} on their
rediscovery of differential cryptanalysis.  The goal of DC is to
recover the key used for a session, so it is potentially far more
serious than the birthday attack.

The book says, "An interesting feature of the new attack is that it
can be applied with the same complexity and success probability even
if the key is frequently changed and thus the collected ciphertexts
are derived from many different keys."  That's pretty alarming.  This
appears in a paragraph discussing chosen plaintext, so it may be
restricted to that case.  I infer from this that rather than needing
the cumulative accretion of information, each trial is independent.

The attack works with either \emph{chosen} plaintext (in which the attacker
says, "Please encrypt this message for me, and give me the
ciphertext,") or \emph{known} plaintext (in which the attacker knows that
the message is a repetition of "HEILHILTER", as figured prominently in
the cracking ofo the Enigma machine in WWII; modern HTTPS connections and
SMTP (email) connections have enough predictability in their content
to provide a similar fulcrum for such a lever; see Sec. 2.x of these
notes).  There is a substantial difference in the complexity of the
two attacks (see Tab. 2.1 in the book).  Known plaintext takes *waaay*
more trials to succeed.


The key idea is the construction of \emph{differentials} (hence the name)
from specific S boxes.  Take two possible inputs to an S box, $X_1$
and $X_2$.  We can assume the subkey has been XORed into both $X_1$
and $X_2$ already.  $Y1$ and $Y2$ are the corresponding outputs of the
S box.  We know that if $X_1 = X_2$, then $Y_1 = Y_2$ and also $X_1 +
X_2 = 0$ (again, here '+' is addition modulo two, or XOR).

For the total encryption algorithm, changing one bit in the input
should result in about half the output bits changing.  The S box
should be similar; small changes in the input should mean large
changes in the output.  In fact, the S box is small enough that we can
exhaustively analyze its inputs.  We also know some rules that were
chosen during the design phase, for example, changing \emph{one} bit in the
six-bit input should result in changes to at \emph{two} bits in the
four-bit output.

So a differential table for each S box is constructed by listing all
$2^6$ possible XORs $X_1 + X_2$, and collecting the stats on $Y_1 +
Y_2$.  From the differences found here, we can work backwards to find
with "high" probability (slightly higher than completely random, at
any rate) some characteristics of the outputs of the *previous* round.

The overall attack is performed by encrypting a bunch of
randomly-chosen *pairs* of plaintexts (chosen as a pair; first a
completely random one, then a second by XORing in a specific value)
and comparing their ciphertexts until we find an output pair of
ciphertexts that fit comfortably with the difference tables we have
pre-computed for the S boxes.  Repeat until we have built up some
statistics about the right set of bits in the output, and from that we
can take a probabilistic guess at the subkey in the last round.  Based
on that guess, we can narrow down the set of subkeys for the
next-to-last round, rinse and repeat.  It's still a computationally
intensive, tedious process, but much less than brute force.  Roughly,
the probability of getting the ciphertext pairs we need is
proportional to $1/p_D$, where $p_D$ is the differential probability in
the table we are looking for, which may be quite small.
(This seems to me that keeping a history of things you've already
tried would increase the probability of finding a pair you like, so
I'm still puzzled by the assertion above that this works even if the
key is being changed frequently.)

Ultimately, this attack is considered practical against ordinary DES.
Biham and Shamir estimated (p. 8, p. 61) that DES and DES-like
systems, even with the full sixteen rounds, could be broken using
$2^{43}$ to $2^{47}$ chosen plaintext/ciphertext pairs.  With 8-byte
blocks, that's encryption of 64 terabytes up to a petabyte.  If the
system under attack can encrypt a 40Gbps link at line rate, that's
only a few hours up to a couple of days of data.

Triple-DES would be much, much longer, but 3-DES is still considered
obsolete due to the birthday paradox attack above.  AES is stronger
against DC, so this attack is of less help against
properly-implemented, modern systems.

I know this is a very rough explanation; I have only a wobbly
understanding of the process myself!  The differential tables are
pretty straightforward, once you understand them, but working from
there to a full-on attack is a big jump.

\subsection{Linear Cryptanalysis}

Linear cryptanalysis (LC) is a known plaintext attack, developed by
Mitsuru Matsui in 1990 after being inspired by differential
cryptanalysis~\cite{matsui1993linear}.  The key(!)  idea is to build a
linear approximation of an S box, allowing you to calculate possible
keys in reverse more quickly.

If a cipher is good, every individual bit of the output should have a
50\% probability of being 0 and a 50\% probability of being 1.
Likewise, there should be no obvious relationship between the input
and output bits (e.g., "If the third input bit is 1, the seventh
output bit is 0.")  However, it is possible that some \emph{combinations}
of bits don't appear with equal probability. For example, the bit
string composed of the first and third input bits and the second and
fourth output bits should have all sixteen combinations 0000, 0001,
..., 1111 with equal probability, but perhaps there is a bias.  If $P_i$
is the $i$th input plaintext bit and $C_i$ is the $i$th output ciphertext
bit, we can construct an equation like

$L = P_1 + P_3 + C_2 + C_4$

(where '+' is modulo 2 addition, or XOR, here).  We say that such a
combination has a bias if the probability of $L$ being 0 is noticeably
different from 50\%.

Such a combination is a \emph{linear} combination of bits.  It is known (by
whom?) that if the S-boxes in our cipher are fully linear, then the
cipher can be easily broken.  Therefore, designers always use
nonlinear S-boxes, but some bias such as this may be discoverable.

The basic idea of LC, then, is to find such sets of bits that exhibit
some bias and use some algebra to recover a few bits of the subkey
used in the last round of the cipher, rinse and repeat.  The trick is
to find the right relationships showing a detectable bias, as was done
with differential analysis.  This is done by examining the math of the
S-boxes in detail; as far as I can tell, this phase of the operation
is done by very smart humans.

If you can find a combination with a bias of $\epsilon$, then you need
$1/\epsilon^2$ plaintext-ciphertext pairs to find some bits of the
subkey.

This is done by taking multiple expressions like the above, combining
them using Matsui's "Piling Up Principle" where you track the biases
multiplied together to make a linear approximation of the nonlinear
S-box.

With this linear approximation, it is possible to find correlations
between the output of the \emph{next-to-last} round of the cipher and the
original input qubits, from which it is possible to recover
information about the \emph{subkey} used in the last round of the cipher.

Ultimately, this doesn't give you complete information about the key
or the plaintext, but substantially reduces the work needed to find
the full key by guiding a search, rather than just testing keys at
random.

Linear cryptanalysis is considered to be a very general technique, but
I don't see extensive attempts to apply it to AES.  Indeed, AES (which
was developed in the 1990s from the proposed cipher known as Rijndael)
was developed specifically with the idea of not being vulnerable to
either DC or LC.

I found Heys' tutorial to be clear and helpful in understanding LC.

\subsection{Known and chosen plaintexts in real systems}

\outlinecomment{Parts of this will make more sense after getting
  through the below.  Maybe this section should be moved below the
  next section.  Also, some pictures will definitely help here.}

\comment{This is fairly ad hoc, a set of rough notes I wrote up
  explaining some ideas without enough rigor.  Would love to have
  someone with more sense than me look this section over.}

Modern HTTPS (web) and SMTP (email) connections have a lot of
predictability in their content, with commands like 'HELO' and
'HTTP/1.1' being standard parts of an exchange.  In
Sec.~\ref{sec:tls}, we'll see more detail about how this encryption is
done using a protocol called Transport Layer Security, or TLS, but for
the moment we'll only focus on the message contents and the fact that
they are pretty predictable.  Thus, it's reasonable to consider
attacks on TLS to be known-plaintext attacks, and in fact there are
cases where we can create chosen-plaintext attacks.

Consider, for example, the connection between your laptop and your
email server, whether Gmail or your organization's server.  Assume
that I, an attacker, can send you email and can observe your encrypted
connection to your server (perhaps I control a router somewhere
between your server and your machine).  I can send you an email
message that contains, for example, the strings 0x000000000000000 (15
zero bytes in a row) and 0x000000010000000 (with a one in the middle).
If your cipher block size is 8 bytes, as in DES, I know that one of
the encrypted blocks will be all zeroes and one will have exactly one
bit set, even if I have trouble controlling the exact position within
the overall stream.  I capture the ciphertext blocks, and compare
them.  This single-bit difference between two blocks helps me with the
attack.  All I have to do to execute a basic chosen-plaintext attack
is to send you email and watch the resulting packets flow between your
machines!

The success of such an attack requires a lot of assumptions about
which parts of the entire process I can observe and which I can
control, but using the principle of being conservative on security,
assuming an attacker can force the choice of plaintext passed between
two nodes through an encrypted connection is not unreasonable in
today's richly interwoven distributed systems.

Especially for IPsec (Sec.~\ref{sec:ipsec}), there is another big
vulnerability: one encrypted connection between two gateways (known as
a \emph{Security Association}, which we will see below) may carry data
encrypted for a \emph{bunch} of machines.  So if an attacker manages
to install a program on only one laptop (say, via email, or while
you're sitting at Starbucks), they can cause your system to send out
arbitrarily chosen packets that will cross the tunnel, so they can
execute a chosen plaintext attack pretty easily.  Since IPsec encrypts
the whole packet, they may not be able to tell immediately which
packets came from your laptop and which from a colleague's laptop, but
that distinction is a relatively minor overhead.

Also, for every IP packet from your laptop to the email server passing
through the IPsec tunnel, the IP header portion is going to be exactly
the same, and its position in the encrypted stream is very easy to
identify.  This led to some of the decisions around the use of CBC, I
believe; I'm not aware of any deeper features intended to further
obscure the location of such predictable data.

In short, as a defender, you should work on the assumption that a
noticeable fraction of your plaintext is known under benign
circumstances, and that it's not all that hard for an attacker to
mount a chosen plaintext attack.

\subsection{Notes \& References}

Some of the references I used for this section:

The best source on the current state of the birthday paradox is Bhargavan and Leurent~\cite{bhargavan2016practical}.  Other references include Miessler~\footnote{\url{https://danielmiessler.com/study/birthday_attack/}} and Wolfram Mathworld~\footnote{\url{http://mathworld.wolfram.com/BirthdayAttack.html}}.

1. Howard M. Heys, "A tutorial on linear and differential
cryptanalysis", undated but probably 1999ish?
\url{http://www.engr.mun.ca/~howard/Research/Papers/ldc_tutorial.html}

2. Abdalla and Bellare,
\url{https://link.springer.com/chapter/10.1007/3-540-44448-3_42}
\url{http://cseweb.ucsd.edu/~mihir/papers/rekey.html}
found via
\url{http://cseweb.ucsd.edu/~mihir/}

That paper talks about differential/linear cryptanalysis and about the
birthday paradox, saying block size $k$ needs to be rekeyed every
$2^{k/2}$ blocks.

Bellare et al, A concrete security treatment of symmetric encryption:
analysis of the DES modes of operation
abstract from STOC 1997
https://ieeexplore.ieee.org/abstract/document/646128
full paper at
\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.4734&rep=rep1&type=pdf}
Focuses on DES, which was in the process of being superseded by AES
even in 1997, but the content of the paper is valuable with respect to
CBC.  I found the paper a tough read when trying to figure out how to
apply the equations.

